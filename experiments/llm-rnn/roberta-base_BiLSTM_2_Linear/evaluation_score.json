{
    "boundary": {
        "precision": 0.0,
        "recall": 0.0,
        "num_correct": 0,
        "num_predicted": 294,
        "num_true": 312,
        "f1": 0
    },
    "classification": {
        "precision": 0,
        "recall": 0,
        "f1": 0,
        "accuracy": 0,
        "support": 0
    },
    "scenario_args": {
        "model_type": "roberta",
        "model_name_or_path": "roberta-base",
        "encoder": "lstm",
        "max_seq_length": 128,
        "num_layers": 2,
        "train_batch_size": 16,
        "eval_batch_size": 16,
        "learning_rate": 2e-05,
        "train_epochs": 9,
        "scenario": "llm-rnn",
        "output_dir": "experiments/llm-rnn/roberta-base_BiLSTM_2_Linear",
        "use_crf": false,
        "bidirectional": true,
        "seed": 42,
        "save_model": false
    }
}